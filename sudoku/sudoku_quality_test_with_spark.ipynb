{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sodoku Quality Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lit, monotonically_increasing_id\n",
    "from pyspark.sql.session import SparkSession\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from helpers.data_prep_and_print import print_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "       .builder   \n",
    "       .master(\"local[*]\")\n",
    "       .appName(\"Sudoku Quality Test\")\n",
    "       .config(\"spark.driver.memory\", \"8g\") \\\n",
    "       .getOrCreate())\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "input_file = path+\"sudoku_1_mio_sequenced_columns.csv\"\n",
    "# load data file.\n",
    "# create a DataFrame using an infered Schema \n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .option(\"delimiter\", \",\") \\\n",
    "       .csv(input_file)\n",
    "df.printSchema()\n",
    "#df = df.limit(10)\n",
    "# alternativ \n",
    "#df = df.sample(0.00001,5)\n",
    "print_df(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All cells are filled => no Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All numbers must be between 0 and 9 for the quizzes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_conditions = []\n",
    "# Iterate over all quizzes col names \n",
    "for col_name in df.columns[:81]:\n",
    "    query_conditions.append (col_name + \" >= 0 AND \" + col_name+ \" <= 9\")\n",
    "df = df.where(\" AND \".join(query_conditions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions must contain numbers between 1 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_conditions = []\n",
    "# Iterate over all col names with solutions \n",
    "for col_name in df.columns[81:]:\n",
    "    query_conditions.append (col_name + \" > 0 AND \" + col_name + \" <=9\")\n",
    "df = df.where(\" AND \".join(query_conditions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All cells != 0 must have identical content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Query\n",
    "query_conditions = [] \n",
    "num_cols = int(len(df.columns)/2 )\n",
    "for i in range(num_cols):\n",
    "    query_conditions.append (df.columns[i]+ \" > 0 AND \" + df.columns[i] +\" == \"+df.columns[i+num_cols])\n",
    "df = df.where(\" OR \".join(query_conditions))\n",
    "print_df(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Solutions: Checking all Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking each row\n",
    "#start = 81\n",
    "df_only_solutions = df[df.columns[81:]].toPandas()\n",
    "\n",
    "def is_in_range_and_unique(values, min_val, max_val):\n",
    "    \"\"\"Check if the list given contains only unique elements\"\"\"\n",
    "    if len(values) == len(set(values)):\n",
    "        return min(values) == min_val and max(values)== max_val \n",
    "    return False\n",
    "\n",
    "def check_unique_num_in_row_and_col(row_as_series):\n",
    "    \"\"\"Input: pd.Series, Output boolean\"\"\"\n",
    "    for start in range(0,row_as_series.size,9):\n",
    "        if not is_in_range_and_unique (row_as_series[start:start+9].to_list(),1,9):\n",
    "            return False\n",
    "    for start in range(0,9):\n",
    "        list_of_indexes = [*range (start,row_as_series.size,9)]\n",
    "        if not is_in_range_and_unique (row_as_series.iloc[list_of_indexes].to_list(),1,9):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "df = df.withColumn(\"ID\", monotonically_increasing_id())\n",
    "result_df = spark.createDataFrame(df_only_solutions.apply(lambda x: check_unique_num_in_row_and_col(x),axis=1).to_frame(\"test_row\").join(df.select(\"ID\").toPandas()))\n",
    "df_joined = df.join(result_df, [\"ID\"])\n",
    "df_joined = df_joined.where(\"test_row == True\")\n",
    "df = df_joined.drop(\"ID\",\"test_row\")\n",
    "print_df(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Solutions: Checking all Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check square\n",
    "# there are nine 3x3 squares \n",
    "# the first square has the indexes (S1A, S1B, S1C, \n",
    "#                                   S2A, S2B, S2C,\n",
    "#                                   S3A, S3B, S3C)\n",
    "\n",
    "def get_square_definition ():\n",
    "    \"\"\"Returns the square positions of a 9*9 sodoku\"\"\"\n",
    "    col_names = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\"] \n",
    "    row_nums = [*range(1,10)]\n",
    "\n",
    "    col_chunks = np.array_split(col_names, 3)\n",
    "    row_chunks = np.array_split(row_nums, 3)\n",
    "    square_pos=[]\n",
    "    for curr_rows in row_chunks :\n",
    "        for curr_cols in col_chunks:\n",
    "            pos_names = []        \n",
    "            for curr_row in curr_rows :\n",
    "                for curr_col in curr_cols:\n",
    "                    pos_names.append(\"S\"+str(curr_row)+str(curr_col))\n",
    "            square_pos.append(pos_names)\n",
    "    return square_pos       \n",
    "\n",
    "def check_squares(row_as_series:pd.Series) -> bool:\n",
    "    \"\"\"Input=pd.Series Output = True/False\"\"\"\n",
    "    for curr_square in get_square_definition() : \n",
    "        if not is_in_range_and_unique(row_as_series.get(curr_square).to_list(),1,9):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "data = df.toPandas()\n",
    "df_only_solutions = data[data.columns[81:]]\n",
    "\n",
    "data[\"test_row\"] = df_only_solutions.apply(lambda x: check_squares(x),axis=1)\n",
    "data = data.query(\"test_row == True\")\n",
    "data = data.drop(columns=[\"test_row\"])\n",
    "df = spark.createDataFrame(data)\n",
    "print_df(df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
